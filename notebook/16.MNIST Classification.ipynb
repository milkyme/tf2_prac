{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 00:25:40.174131: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def get_mnist_dataset():\n",
    "    (train_val_dataset, test_dataset), dataset_info = tfds.load(name = 'mnist',\n",
    "                                                                shuffle_files = True,\n",
    "                                                                as_supervised = True,\n",
    "                                                                split = ['train', 'test'],\n",
    "                                                                with_info = True)\n",
    "    number_train_val = dataset_info.splits['train'].num_examples\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    number_train = int(number_train_val*train_ratio)\n",
    "    number_val = number_train_val - number_train\n",
    "    \n",
    "    train_dataset = train_val_dataset.take(number_train)\n",
    "    val_dataset = train_val_dataset.skip(number_train).take(number_val)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(train_dataset, val_dataset, test_dataset, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
    "    def standard(images, labels):\n",
    "        images = tf.cast(images, tf.float32) / 255.0\n",
    "        return (images, labels)\n",
    "    \n",
    "    train_dataset = train_dataset.map(standard).shuffle(100).batch(TRAIN_BATCH_SIZE)\n",
    "    val_dataset = val_dataset.map(standard).batch(TEST_BATCH_SIZE)\n",
    "    test_dataset = test_dataset.map(standard).batch(TEST_BATCH_SIZE)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = standardization(train_dataset, val_dataset, test_dataset, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Classifier(Model):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Classifier, self).__init__()\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.l1 = Dense(64, activation = 'relu')\n",
    "        self.l2 = Dense(10, activation = 'softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = Mean()\n",
    "val_loss = Mean()    \n",
    "test_loss = Mean()\n",
    "\n",
    "train_acc = SparseCategoricalAccuracy()\n",
    "val_acc = SparseCategoricalAccuracy()\n",
    "test_acc = SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def trainer():\n",
    "    global train_dataset, model, loss_object, optimizer\n",
    "    global train_loss, train_acc\n",
    "    \n",
    "    for images, labels in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images)\n",
    "            loss = loss_object(labels, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validator():\n",
    "    global val_dataset, model, loss_object, optimizer\n",
    "    global val_loss, val_acc\n",
    "    \n",
    "    for images, labels in val_dataset:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        \n",
    "        val_loss(loss)\n",
    "        val_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester():\n",
    "    global test_dataset, model, loss_object, optimizer\n",
    "    global test_loss, test_acc\n",
    "    \n",
    "    for images, labels in test_dataset:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        \n",
    "        test_loss(loss)\n",
    "        test_acc(labels, predictions)\n",
    "        \n",
    "    print('Test Loss: {:.4f}\\t Test Accuracy: {:.2f}%'.format(test_loss.result(), test_acc.result()*100))\n",
    "    # Not use @tf.function if function contains print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reporter(epoch):\n",
    "    global train_loss, train_acc\n",
    "    global val_loss, val_acc\n",
    "    \n",
    "    print('Epoch: ', epoch+1, 'Train Loss: {:.4f}\\t Train Accuracy: {:.2f}%\\t\\\n",
    "    Validation Loss: {:.4f}\\t Validation Accuracy: {:.2f}%'\\\n",
    "    .format(train_loss.result(), train_acc.result()*100, val_loss.result(), val_acc.result()*100))\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 00:25:43.028154: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Train Loss: 0.2532\t Train Accuracy: 92.34%\t    Validation Loss: 0.1793\t Validation Accuracy: 94.72%\n",
      "Epoch:  2 Train Loss: 0.1631\t Train Accuracy: 95.26%\t    Validation Loss: 0.1853\t Validation Accuracy: 94.80%\n",
      "Epoch:  3 Train Loss: 0.1388\t Train Accuracy: 96.03%\t    Validation Loss: 0.2046\t Validation Accuracy: 94.58%\n",
      "Epoch:  4 Train Loss: 0.1316\t Train Accuracy: 96.36%\t    Validation Loss: 0.2053\t Validation Accuracy: 94.90%\n",
      "Epoch:  5 Train Loss: 0.1228\t Train Accuracy: 96.69%\t    Validation Loss: 0.2051\t Validation Accuracy: 95.14%\n",
      "Epoch:  6 Train Loss: 0.1156\t Train Accuracy: 96.88%\t    Validation Loss: 0.2223\t Validation Accuracy: 95.32%\n",
      "Epoch:  7 Train Loss: 0.1105\t Train Accuracy: 97.05%\t    Validation Loss: 0.2230\t Validation Accuracy: 95.24%\n",
      "Epoch:  8 Train Loss: 0.1013\t Train Accuracy: 97.34%\t    Validation Loss: 0.2288\t Validation Accuracy: 95.30%\n",
      "Epoch:  9 Train Loss: 0.1039\t Train Accuracy: 97.39%\t    Validation Loss: 0.2568\t Validation Accuracy: 95.24%\n",
      "Epoch:  10 Train Loss: 0.0986\t Train Accuracy: 97.47%\t    Validation Loss: 0.2657\t Validation Accuracy: 95.16%\n",
      "Test Loss: 0.2796\t Test Accuracy: 95.39%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LR = 0.01\n",
    "\n",
    "model = MNIST_Classifier()\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "optimizer = Adam(learning_rate = LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    trainer()\n",
    "    validator()\n",
    "    train_reporter(epoch)\n",
    "    \n",
    "tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
